{"cells":[{"cell_type":"markdown","metadata":{"id":"intro_markdown"},"source":["# 【ディスク節約・最終版】AI学習ノートブック\n","\n","**目的：RAM不足、接続エラー、ディスク容量不足の3大問題を完全に解決するため、処理済みのチャンクファイルを即座に削除する「追記＆削除」方式を採用します。これが無料版Colabでプロジェクトを完遂させるための最も堅牢なアプローチです。**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"TVxNcKlmrlqp"},"source":["## STEP 1: 環境設定と再接続（最重要）\n","\n","**最初に必ず実行してください。**\n","Google Driveとの接続を確立し、作業場所へ移動します。\n","\n","---\n","**【エラーが起きたら？】**\n","もし、この後のステップでエラーが出た場合は、**慌てずにこのSTEP 1のセルをもう一度実行してください。** 接続がリフレッシュされ、安全に作業を再開できます。\n","---"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"j9PfyuyCrlqq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759322267872,"user_tz":-540,"elapsed":21931,"user":{"displayName":"SEnA -","userId":"07897138932926808776"}},"outputId":"ed208b2c-0d6f-44eb-caff-cce700de0d61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: mahjong in /usr/local/lib/python3.12/dist-packages (1.3.0)\n","Mounted at /content/drive\n","/content/drive/MyDrive/いろいろ/麻雀AI/googlecolab/senasJanAI\n"]}],"source":["!pip install mahjong\n","from google.colab import drive\n","# force_remount=True で、壊れた接続を強制的にリフレッシュします\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Google Drive上のプロジェクトルートディレクトリを定義します\n","GDRIVE_PROJECT_ROOT = \"/content/drive/MyDrive/いろいろ/麻雀AI/googlecolab/senasJanAI\"\n","\n","# プロジェクトのルートディレクトリに移動します\n","%cd {GDRIVE_PROJECT_ROOT}"]},{"cell_type":"markdown","metadata":{"id":"append_delete_merge_markdown"},"source":["## STEP 2: 【追記＆削除方式】チャンクを結合しては削除\n","\n","**STEP 1の完了後、このセルを実行してください。**\n","チャンクファイルを一つずつバイナリファイルに追記し、成功したら即座に元のチャンクファイルを削除します。これにより、ディスク使用量を増やすことなく処理を進めます。\n","\n","**（注意：完了までには非常に長い時間がかかりますが、放置しておけばいつか終わります）**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"append_delete_merge_code","colab":{"base_uri":"https://localhost:8080/"},"outputId":"34674322-dce1-43cd-adbd-cddfc48b7919"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/いろいろ/麻雀AI/googlecolab/senasJanAI\n","--- Starting Append-and-Delete Merging Process ---\n"]}],"source":["import glob, pickle, numpy as np, os, tensorflow as tf\n","\n","#【安全対策】実行前に必ずプロジェクトルートに移動します\n","%cd {GDRIVE_PROJECT_ROOT}\n","\n","# 各種パスを設定します\n","GDRIVE_PROCESSED_DIR = os.path.join(GDRIVE_PROJECT_ROOT, 'processed_data')\n","CHUNK_DIR_GDRIVE = os.path.join(GDRIVE_PROCESSED_DIR, 'chunks')\n","MERGED_LOG_PATH = os.path.join(GDRIVE_PROCESSED_DIR, 'merged_chunks.log')\n","CONTEXTS_BIN_PATH = os.path.join(GDRIVE_PROCESSED_DIR, 'contexts.bin')\n","CHOICES_BIN_PATH = os.path.join(GDRIVE_PROCESSED_DIR, 'choices.bin')\n","LABELS_BIN_PATH = os.path.join(GDRIVE_PROCESSED_DIR, 'labels.bin')\n","\n","print(\"--- Starting Append-and-Delete Merging Process ---\")\n","all_chunk_files = sorted(glob.glob(os.path.join(CHUNK_DIR_GDRIVE, \"*.pkl\")))\n","\n","if not all_chunk_files:\n","    print(\"No chunk files found. Assuming merge is complete. Proceed to the next step.\")\n","else:\n","    # 既にマージ済みのファイルリストを読み込む\n","    merged_files = set()\n","    if os.path.exists(MERGED_LOG_PATH):\n","        with open(MERGED_LOG_PATH, 'r') as f:\n","            merged_files = set(line.strip() for line in f)\n","\n","    files_to_merge = [f for f in all_chunk_files if f not in merged_files]\n","\n","    if not files_to_merge:\n","        print(\"All chunks have already been merged! Proceed to the next step.\")\n","    else:\n","        print(f\"{len(merged_files)} chunks already processed. Resuming with {len(files_to_merge)} remaining chunks.\")\n","\n","        progbar = tf.keras.utils.Progbar(len(files_to_merge), unit_name=\"chunk\")\n","        with open(MERGED_LOG_PATH, 'a') as log_f:\n","            for i, chunk_file in enumerate(files_to_merge):\n","                try:\n","                    # 1. チャンクを読み込む\n","                    with open(chunk_file, 'rb') as f:\n","                        contexts, choices, labels = pickle.load(f)\n","\n","                    # 2. バイナリファイルに追記\n","                    with open(CONTEXTS_BIN_PATH, 'ab') as f_ctx, open(CHOICES_BIN_PATH, 'ab') as f_cho, open(LABELS_BIN_PATH, 'ab') as f_lbl:\n","                        contexts.tofile(f_ctx)\n","                        choices.tofile(f_cho)\n","                        labels.tofile(f_lbl)\n","\n","                    # 3. 成功ログを記録\n","                    log_f.write(chunk_file + '\\n')\n","                    log_f.flush()\n","\n","                    # 4. 元のチャンクファイルを削除\n","                    os.remove(chunk_file)\n","\n","                    progbar.update(i + 1)\n","                except Exception as e:\n","                    print(f\"\\nAn error occurred while processing {os.path.basename(chunk_file)}: {e}\")\n","                    print(\"Please re-run STEP 1 and then this cell to resume.\")\n","                    # エラー発生時は安全のため処理を中断\n","                    raise\n","\n","        print(\"\\n--- Append & Delete Merging Complete! Proceed to Final Formatting. ---\")"]},{"cell_type":"markdown","metadata":{"id":"final_format_markdown"},"source":["## STEP 3: 最終データ整形 ＆ データセット完成\n","\n","**STEP 2が完了したら、このセルを実行してください。**\n","追記して作成した巨大なバイナリファイルを読み込み、最終的な学習データセットを完成させます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"final_format_code"},"outputs":[],"source":["import pickle, numpy as np, os\n","from src.transformer.vectorizer import MAX_CONTEXT_LENGTH, MAX_CHOICES, VECTOR_DIM\n","\n","#【安全対策】実行前に必ずプロジェクトルートに移動します\n","%cd {GDRIVE_PROJECT_ROOT}\n","\n","# 各種パスを設定します\n","GDRIVE_PROCESSED_DIR = os.path.join(GDRIVE_PROJECT_ROOT, 'processed_data')\n","FINAL_DATASET_PATH_GDRIVE = os.path.join(GDRIVE_PROCESSED_DIR, 'training_dataset_transformer.pkl')\n","CONTEXTS_BIN_PATH = os.path.join(GDRIVE_PROCESSED_DIR, 'contexts.bin')\n","CHOICES_BIN_PATH = os.path.join(GDRIVE_PROCESSED_DIR, 'choices.bin')\n","LABELS_BIN_PATH = os.path.join(GDRIVE_PROCESSED_DIR, 'labels.bin')\n","\n","print(\"--- Starting Final Data Formatting ---\")\n","if not os.path.exists(LABELS_BIN_PATH):\n","    print(\"Error: Merged binary file not found. Please complete STEP 2 first.\")\n","else:\n","    # バイナリファイルからnumpy配列として読み込む\n","    print(\"Loading binary data files...\")\n","    contexts_flat = np.fromfile(CONTEXTS_BIN_PATH, dtype=np.float32)\n","    choices_flat = np.fromfile(CHOICES_BIN_PATH, dtype=np.float32)\n","    labels_final = np.fromfile(LABELS_BIN_PATH, dtype=np.int32)\n","\n","    # 正しい形状にリシェイプ\n","    total_samples = len(labels_final)\n","    contexts_final = contexts_flat.reshape(total_samples, MAX_CONTEXT_LENGTH, VECTOR_DIM)\n","    choices_final = choices_flat.reshape(total_samples, MAX_CHOICES, VECTOR_DIM)\n","\n","    print(f\"Loaded and reshaped data with {total_samples} samples.\")\n","    print(\"Generating final masks...\")\n","\n","    non_zero_counts = np.count_nonzero(np.sum(choices_final, axis=2), axis=1)\n","    masks = np.zeros((total_samples, MAX_CHOICES), dtype='float32')\n","    for i, count in enumerate(non_zero_counts): masks[i, :count] = 1.0\n","\n","    final_dataset = (contexts_final, choices_final, labels_final, masks)\n","\n","    print(f\"Saving final dataset to Google Drive: '{FINAL_DATASET_PATH_GDRIVE}'...\")\n","    with open(FINAL_DATASET_PATH_GDRIVE, 'wb') as f:\n","        pickle.dump(final_dataset, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    print(\"\\n--- Final Dataset Created Successfully! --- AIの学習に進んでください。 ---\")\n","\n","    # 一時ファイルをクリーンアップ\n","    MERGED_LOG_PATH = os.path.join(GDRIVE_PROCESSED_DIR, 'merged_chunks.log')\n","    !rm -f \"{CONTEXTS_BIN_PATH}\"\n","    !rm -f \"{CHOICES_BIN_PATH}\"\n","    !rm -f \"{LABELS_BIN_PATH}\"\n","    !rm -f \"{MERGED_LOG_PATH}\"\n","    print(\"Cleaned up temporary files.\")"]},{"cell_type":"markdown","metadata":{"id":"6t2-RZcLrlqt"},"source":["## STEP 4: AIモデルの学習\n","\n","**STEP 3が完了したら、このセルを実行してください。**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGpX1gsRrlqu"},"outputs":[],"source":["import os\n","\n","#【安全対策】実行前に必ずプロジェクトルートに移動します\n","%cd {GDRIVE_PROJECT_ROOT}\n","\n","# train_transformer.pyをDrive上で動作するように、パスを書き換えます\n","with open('src/transformer/train_transformer.py', 'r') as f:\n","    code = f.read()\n","code = code.replace(\"PROCESSED_DATA_PATH = '/content/processed_data/training_dataset_transformer.pkl'\", f\"PROCESSED_DATA_PATH = '{os.path.join(GDRIVE_PROJECT_ROOT, 'processed_data', 'training_dataset_transformer.pkl')}'\")\n","code = code.replace(\"MODEL_PATH = '/content/models/senas_jan_ai_transformer_v1.keras'\", f\"MODEL_PATH = '{os.path.join(GDRIVE_PROJECT_ROOT, 'models', 'senas_jan_ai_transformer_v1.keras')}'\")\n","code = code.replace(\"os.makedirs('/content/models', exist_ok=True)\", f\"os.makedirs('{os.path.join(GDRIVE_PROJECT_ROOT, 'models')}', exist_ok=True)\")\n","with open('src/transformer/train_transformer_drive.py', 'w') as f:\n","    f.write(code)\n","\n","print(\"--- Starting Training ---\")\n","!python -m src.transformer.train_transformer_drive"]},{"cell_type":"markdown","metadata":{"id":"TUtqnx_Wrlqu"},"source":["## STEP 5: 学習済みAIによる予測\n","\n","**学習が完了したら、このセルを実行してください。**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_49-A_Frlqu"},"outputs":[],"source":["import os\n","\n","#【安全対策】実行前に必ずプロジェクトルートに移動します\n","%cd {GDRIVE_PROJECT_ROOT}\n","\n","# predict_transformer.pyをDrive上で動作するように、パスを書き換えます\n","with open('src/transformer/predict_transformer.py', 'r') as f:\n","    code = f.read()\n","code = code.replace(\"MODEL_PATH = '/content/models/senas_jan_ai_transformer_v1.keras'\", f\"MODEL_PATH = '{os.path.join(GDRIVE_PROJECT_ROOT, 'models', 'senas_jan_ai_transformer_v1.keras')}'\")\n","with open('src/transformer/predict_transformer_drive.py', 'w') as f:\n","    f.write(code)\n","\n","!python -m src.transformer.predict_transformer_drive"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}